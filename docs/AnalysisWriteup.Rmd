---
title: "A QTL mapping analysis using the r/qtl and targets packages (Work in progress)"
author: "Jay Gillenwater"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
## target knits Rmds in their own session, so load libraries here.
## source("../packages.R")

local({
  hook_output <- knitr::knit_hooks$get('output')
  knitr::knit_hooks$set(output = function(x, options) {
    if (!is.null(options$max.height)) options$attr.output <- c(
      options$attr.output,
      sprintf('style="max-height: %s;"', options$max.height)
    )
    hook_output(x, options)
  })
})

```


Working on projects in R is typically an iterative experience. You write a function, record the results, explore the results with plots, improve your functions, rinse and repeat. Repeat for just a little while and you end up with an environment filled with forgotten names and a script half a mile long (speaking from personal experience). 

This iterative process is especially true for QTL mapping experiments in R. Even with high-quality data in hand, it may not be obvious what the proper mapping technique you should use, or what parameters should be used for a given technique. The process of choosing the appropriate model and model parameters has to instead be tailored to the organism, trait, and structure of the data for each experiment on a case-by-case basis.

In practice, this involves fitting many models, and inspecting the results of each model and how they compare to one another. Keeping track of the models alone can quickly become difficult, let alone how they compare with one another.

One option to alleviate some of these challenges is to use workflow management software which provide an organizational framework for projects in R (and other languages). One such package in R is [targets](https://github.com/ropensci/targets). In the authors words: 

> The targets package is a Make-like pipeline toolkit for Statistics and data science in R. With targets, you can maintain a reproducible workflow without repeating yourself. targets skips costly runtime for tasks that are already up to date, runs the necessary computation with implicit parallel computing, and abstracts files as R objects. A fully up-to-date targets pipeline is tangible evidence that the output aligns with the code and data, which substantiates trust in the results.

In general, using these management tools allow researchers to efficiently manage, understand, and communicate the results of projects that would be frustrating for the programmer to keep track of, and difficult for someone new to the code to understand. Furthermore, these packages are designed to ensure that the results obtained are reproducible so that both you and anyone you share the code with can have confidence in the consistency of the results you obtain. 

The targets package also has an ecosystem of side "helper" packages that augment its base capabilities. These include [tflow](https://github.com/MilesMcBain/tflow) to easily set up projects that use targets, [fnmate](https://github.com/MilesMcBain/fnmate) to define functions in targets projects with simple shortcuts, and the [targetopia](https://wlandau.github.io/targetopia/packages.html) collection of target-adjacent packages for more specialized applications. 

Beyond providing an organizational framework, targets steers users towards a workflow which makes heavy use of functional programming. This allows users to create analysis plans that are both compact and very flexible.

I've become very fond these packages, and end up using them in just about every project I do beyond the most trivial of jobs. The goal of this document is to both show how to use the targets package to implement a QTL mapping pipeline, and demonstrate why I like using it and its related packages so much. 

[This blog post](https://www.milesmcbain.com/posts/the-drake-post/) by the author of the tflow and fnmate packages is an especially good (and accessible) overview of the benefits of using these workflows, and how to implement them. It almost single-handedly convinced me to start using these packages. The [drake](https://github.com/ropensci/drake) package mentioned in the post is however the predecessor of the targets package. The syntax has changed a bit between the two packages, but the overall goals and design has remained largely the same. Theres even [a chapter](https://books.ropensci.org/targets/drake.html) in the targets manual dedicated to explaining the differences between the two.

Some other potentially helpful links are [this general overview](https://ropensci.org/blog/2021/02/03/targets/)

## QTL Mapping

Before I go into the actual implementation, I'll (briefly) go over a very general QTL mapping workflow. 

### Techniques

r/qtl provides several techniques for performing QTL mapping. Broadly, these may be broken down in terms of those designed to estimate the effects of single QTL, and those designed to estimate the effects of multiple QTL which act simultaneously to influence some trait. In r/qtl the single QTL methods are implemented with the **scanone** function, and the multi-QTL methods are implemented with the **cim**, **stepwiseqtl** , and **mqmscan** functions for composite interval mapping , multiple interval mapping through forward/backward search, and multiple QTL mapping (MQM) respectively. 

A layer of difficulty is added by the multi-QTL models as each has a number of parameters which must be supplied by a user and it is not always obvious what the best choice should be. As an example, in composite interval mapping, a user must supply the number of marker cofactors to use in the analysis, and a window size (in cM). Appropriate values to choose for these parameters is not always clear, and it is typically best to try different combinations, and compare the results you get. 

### Significance Thresholds

Once a scan is performed with some technique, a user then has to decide what qualifies as a *significant* QTL. Evidence for a QTL is typically given as a logarithm of odds (LOD) score where higher values indicate more evidence in favor of the presence of a QTL at some location. To determine an appropriate threshold for declaring significance, a permutation test is typically performed. This involves randomizing the phenotype data against the genotype data, performing a scan for QTL, and recording the highest LOD score obtained. By repeating this procedure many times, a user can generate a distribution of LOD scores that you would expect to see if there was no association between the phenotypes and genotypes **for your specific data set**. A user can the select a percentile of this distribution and declare any QTL with a LOD score above that threshold to be significant. 

### Estimating effects

Once a set of significant QTL has been found, the next step is to obtain information about the QTL themselves. This often includes statistics like how much of the phenotypic variation can be explained by the QTL, its additive effect, a LOD score, and others. In r/qtl this information is obtained with the **fitqtl** function.  

### General workflow

Below I made a simple flowchart to show the general steps for the QTL mapping analysis. These are the general steps that I will incorporate into the workflow I implement with targets. In practice, the analysis flow chart itself will be more complicated, but I'll get to that in a bit.   


```{r QTLFlowchart, echo=FALSE, fig.dim = c(10,10), fig.align = 'center'}
tar_load(QTL_Flowchart) 
QTL_Flowchart
```

You can see that the main differences between the single and multi-QTL mapping paths stem from the fact that you have to evaluate the effect of the different parameters supplied to the model. This isn't to say that using the single-QTL mapping techniques don't involve specifying important model parameters, but in most cases, a researcher will have a good idea what an appropriate choice would be from the onset of the experiment in contrast to the parameters for the multi-QTL techniques that are more subjective. 

## QTL mapping with targets

In this section, I'm going to talk about some functionality of targets that have been really useful for running QTL analyses. While I'll try to go into detail on some of the most important functions for this, I wont be able to cover all the inns and outs of targets, and running a targets project. That being said, the package authors have written a very good [user manual](https://books.ropensci.org/targets/) to go over all the fine details of using the package. In case you haven't read [this blog post](https://www.milesmcbain.com/posts/the-drake-post/) yet, do it. It was really helpful for getting me up and running with drake, and the concepts carry over almost seamlessly to targets. I'd suggest that if you haven't used the package yet, start with the blog post and then check out the manual. 

With the general QTL mapping flowchart above, the next part is how to actually go about implementing it in code. Translating a conceptual flowchart like the one above to code in targets is actually (relatively) straightforward. Briefly, targets requires you to have a specially formatted "master" script file that tells R what tasks you want to perform, and how you will perform them. The "what" part is just a name, and the "how" part given through function calls. A target is essentially just a name a function, and a workflow is a series of these targets that you tie together by using the output of one target as the input to another. In terms of the flowchart above, the "what" is the name of the shapes, and the "how" is what you do to move from one shape to the next. targets then works by analyzing the code in your master script to build a directed graph of analysis tasks. When you run a workflow, this graph is then used to calculate tasks in the order in which they show up. This graph structure is also what targets uses to keep an analysis up to date. If you change a function that is used in an early target, you can see what output is affected by seeing what downstream targets are connected to the target that uses the changed function as a command.  

There's a few ways to go about implementing a workflow in targets, but I like using the **tar_plan** function which comes from the [tarchetypes](https://github.com/ropensci/tarchetypes) package. Briefly, this function wraps a series of target definitions using a special format.  To give an example, here's what the QTL mapping plan looks like with just the first four targets. **tar_target** is the function you use to define a target. The first argument to the function is the name of the target, the second is the "command", what the target needs to do. There's many other available arguments, but you can do a whole lot with just these two. 

For now, ignore all the text in the script before the **tar_plan** call. In the plan below, the first target is named **InputFile**. It just defines where I have the .csv cross stored on my computer. The following targets read in the cross, perform QTL mapping with single marker regression and perform permutations with single marker regression. There's a couple things to note here. First, you can see the special **format** argument in the first target. This lets the workflow know that this target is an external file. In the second target, I used a different way to name the target. This is the naming scheme from **drake** where you assign the output of the target command to the target name. In practice, this is equivalent too the following two target definitions which have the name in the first argument position of the function, and the command given by the custom function call in the second position. 

<br>
<details><summary>A simple plan (click the triangle to show/hide)</summary>

```{r, First_Targets, eval = FALSE, echo = TRUE}
## Load your packages, e.g. library(targets).
source("./packages.R")

## Load your R files
lapply(list.files("./R", full.names = TRUE), source)

## tar_plan supports drake-style targets and also tar_target()
tar_plan(
  
  # The input cross data, already in the r/qtl "csv" format
  tar_target(InputFile, 
             "Data/ExampleCross.csv",
             format = "file"),
  
  # Read the QTL file in with r/qtls "read.cross" function. Also apply
  # a few basic operations to clean the initial cross
  QTLData = read_cross(QTL_File = InputFile),
  
  # Marker regression mapping
  tar_target(RegressionResults,
             Map_Regression(CrossData = QTLData,
                                Pheno = phenames(QTLData))),
  
  # Permutations for the marker regression
  tar_target(RegressionPerms,
             Map_Regression_Perms(CrossData = QTLData,
                                  Pheno     = phenames(QTLData),
                                  nPerm     = 100))
  
)
```
</details>
<br>

To give more detail, look at what the (barely) custom **Map_Regression** function from the third target (RegressionResults) looks like. 

<br>
<details><summary>marker regression function</summary>

```{r, MapRegressionDef, eval = FALSE, echo = TRUE}
Map_Regression <- function(CrossData = QTLData, Pheno = phenames(QTLData)) {

  # Just a wrapper for scanone with the method set, and arguments 
  # supplied in names defined in the workflow
  scanone(cross     = CrossData, 
          pheno.col = Pheno,
          method    = "mr")

}

```
</details>
<br>

You can see why I say "barely". This function is just a wrapper for the scanone function from r/qtl with the method set to marker regression, and slightly different argument names. You could just as easily use the scanone function alone, I only wrapped the function so that it would be more obvious what I was doing when I read through my plan. 

Now would be a good time to introduce another helpful function. At any point, you can see the underlying graph that drives the workflow with the **tar_visnetwork** function. Here is what this network looks like right now with just these four targets. 


```{r, SimpleNet, results="asis"}
cat( paste( readLines( paste0(here(), "/NetworkPlots/SimpleNetwork.html") ), collapse="\n" ) )
```

This is a pretty straightforward workflow, and it's not very different than what you would get by writing a standard R script. Things get more interesting when we start to fit many models, and compare their results. 

The **cim** function from r/qtl is used to perform composite interval mapping. This function has two main parameters: the number of marker cofactors, and the window size. When performing a qtl mapping experiment, you might want to try different combinations of these parameters and inspect the results of each combination. This is where targets really starts to shine. 

Here's what the plan looks like after I added some functions to perform composite interval mapping with many different parameter combinations. 

<br>
<details><summary>The plan with composite interval mapping added</summary>

```{r, CIMPlan, eval = FALSE, echo = TRUE}
## Load your packages, e.g. library(targets).
source("./packages.R")

## Load your R files
lapply(list.files("./R", full.names = TRUE), source)

AllPhenos <- c("EarHeight",
               "LeafLength",
               "LeafWidth",
               "CobWeight",
               "TotalKernelVolume")

# Combinations of parameters for composite interval mapping.
# Useful early on for checking the effects of different model parameters on the output
CIM_ModelParams <- expand_grid(
  CIM_n.marcovar = c(5, 7, 10),
  CIM_Window     = c(10, 15, 20),
  PhenoNames     = AllPhenos
)

## tar_plan supports drake-style targets and also tar_target()
tar_plan(
  
  # The input cross data, already in the r/qtl "csv" format
  tar_target(InputFile, 
             "Data/ExampleCross.csv",
             format = "file"),
  
  # Read the QTL file in with r/qtls "read.cross" function. Also apply
  # a few basic operations to clean the initial cross
  QTLData = read_cross(QTL_File = InputFile),
  
  # Marker regression mapping and permutations
  tar_target(RegressionResults,
             Map_Regression(CrossData = QTLData,
                                Pheno = phenames(QTLData))),
  
  tar_target(RegressionPerms,
             Map_Regression_Perms(CrossData = QTLData,
                                  Pheno     = phenames(QTLData),
                                  nPerm     = 100)),
  

  # Simple interval mapping and permutations
  tar_target(SimpleIntervalResults,
             Map_SimpleInterval(CrossData   = QTLData,
                                      Pheno = phenames(QTLData))),
  
  tar_target(SimpleIntervalPerms,
             Map_SimpleInterval_Perms(CrossData = QTLData,
                                      Pheno     = phenames(QTLData),
                                      nPerm     = 100)),
  
  # Composite interval mapping
  # Perform composite interval mapping with each combination of phenotype,
  # number of marker covariates, and window size as specified above
  CIM_Mapped <- tar_map(
    
    values = CIM_ModelParams,
    
    tar_target(CompositeIntervalResults,
               Map_Composite(CrossData  = QTLData,
                             Pheno      = PhenoNames,
                             nCovar     = CIM_n.marcovar,
                             WindowSize = CIM_Window,
                             perm       = F))
    
  )
)

```
</details>
<br>

A first thing to notice are the **AllPhenos** vector and the **CIM_ModelParams** tibble before the call to **tar_plan**. AllPhenos is a vector which says what phenotypes I want to perfrom QTL mapping on, and the **CIM_ModelParams** is a table that holds every combination of model parameters (number of marker cofactors, window size, and phenotype) that I want to try in composite interval mapping. In this case, I used the expand_grid function to return a tibble with a row for every combination of the three paramater vectors. I supplied three numbers of marker cofactors, three window sizes, and 5 phenotypes, so this table has a total of 45 (3 x 3 x 5) rows; one for each unique parameter combination. 

Normally, fitting a model for each of these combinations would be relatively cumbersome to implement in base R, and even harder to keep the results organized. In targets, a solution to this repetition of the same analysis task with different parameter combinations is already provided through [branching](https://books.ropensci.org/targets/static.html). In short, this allows you to supply a target with a data structure that holds *named* parameter values which can then be accessed by a function in a target that has arguments which refer to each parameter value by its name in the data structure. 

A user can then define a [pattern](https://books.ropensci.org/targets/dynamic.html#patterns) when using dynamic branching, or use helper functions from [tarchetypes](https://docs.ropensci.org/tarchetypes/reference/index.html) when using static branching to describe how the values in this data structure should be used to create a whole set of targets. 

In that plan above, I give the parameter combinations in the **CIM_ModelParams** tibble up front, and actually define the new targets in the **CIM_Mapped** step. The **tar_map**  function I use there is a special helper function from tarchetypes. This uses similar logic to the **map** function from [purrr](https://purrr.tidyverse.org/reference/map.html) where the function you give in the target definition(s) is applied to each parameter(s) you provide with the value argument. In the example above, you can see that the **CompositeIntervalResults** target calls the **Map_Composite** function which takes as arguments a cross object, a number of marker cofactors, a window size, and a phenotype. The values given for these arguments match the names of the paramaters given in the **CIM_ModelParams** tibble, with the exception of the cross object which is provided through the name of the upstream target that holds the cross. This is how targets figures out which targets need to be created, and what commands (function calls) need to be performed for each. 

At this point, it may be helpful to check up on what the graph looks like now. 
<br>
```{r, CIMNet, results="asis"}
cat( paste( readLines( paste0(here(), "/NetworkPlots/CIMPlot.html") ), collapse="\n" ) )
```
<br>

With just a few lines, you can create a huge number of new analyses. By scrolling in on that graph, you can also see that targets has named each target according to the parameter values that were mapped to each function call. As an example, the target names **CompositeIntervalResults_5_15_LeafLength**
gets its name from **TargetName_"CIM_n.marcovar"_"CIM_Window"_PhenoNames** the order of the paramaters min the target name matches the order of the names in the value object, in this case the **CIM_ModelParams** tibble. 

At this point, the value of any one target can be read by calling tar_read("Target name"). This is helpful for checking that you are getting the output you'd expect, but again gets very cumbersome when you are woring with a large number of targets. A straightforward solution to this is to use another helper function, **tar_combine**. As the name suggests, this function takes several upstream targets, and combines them into a single new target. This function just requires a name, what targets you want to combine, and how you want the results of targets to be combined. 

Here's what the plan and it's graph look like after **tar_combine** is added to the plan. 

<br>
<details><summary>Combining the results</summary>
```{r, CIMPlan_COmbined, eval = FALSE, echo = TRUE}
## Load your packages, e.g. library(targets).
source("./packages.R")

## Load your R files
lapply(list.files("./R", full.names = TRUE), source)

AllPhenos <- c("EarHeight",
               "LeafLength",
               "LeafWidth",
               "CobWeight",
               "TotalKernelVolume")

# Combinations of parameters for composite interval mapping.
# Useful early on for checking the effects of different model parameters on the output
CIM_ModelParams <- expand_grid(
  CIM_n.marcovar = c(5, 7, 10),
  CIM_Window     = c(10, 15, 20),
  PhenoNames     = AllPhenos
)

## tar_plan supports drake-style targets and also tar_target()
tar_plan(
  
  # The input cross data, already in the r/qtl "csv" format
  tar_target(InputFile, 
             "Data/ExampleCross.csv",
             format = "file"),
  
  # Read the QTL file in with r/qtls "read.cross" function. Also apply
  # a few basic operations to clean the initial cross
  QTLData = read_cross(QTL_File = InputFile),
  
  # Marker regression mapping and permutations
  tar_target(RegressionResults,
             Map_Regression(CrossData = QTLData,
                                Pheno = phenames(QTLData))),
  
  tar_target(RegressionPerms,
             Map_Regression_Perms(CrossData = QTLData,
                                  Pheno     = phenames(QTLData),
                                  nPerm     = 100)),
  

  # Simple interval mapping and permutations
  tar_target(SimpleIntervalResults,
             Map_SimpleInterval(CrossData   = QTLData,
                                      Pheno = phenames(QTLData))),
  
  tar_target(SimpleIntervalPerms,
             Map_SimpleInterval_Perms(CrossData = QTLData,
                                      Pheno     = phenames(QTLData),
                                      nPerm     = 100)),
  
  # Composite interval mapping
  # Perform composite interval mapping with each combination of phenotype,
  # number of marker covariates, and window size as specified above
  CIM_Mapped <- tar_map(
    
    values = CIM_ModelParams,
    
    tar_target(CompositeIntervalResults,
               Map_Composite(CrossData  = QTLData,
                             Pheno      = PhenoNames,
                             nCovar     = CIM_n.marcovar,
                             WindowSize = CIM_Window,
                             perm       = F))
    
  ),
  
  # Combine the results to a single list
   tar_combine(CompositeIntervalResults_Combined,
               CIM_Mapped[[1]],
               command = list(!!!.x)),
  
  # A simple flowchart for the general QTL mapping process (used in the writeup)
   tar_target(QTL_Flowchart, 
           make_flowchart()),
  
  # Render the analysis writeup
   tar_render(AnalysisWriteup, "docs/AnalysisWriteup.Rmd")
)

```
</details>
<br>

<br>
```{r, CIMNet_Combined, results="asis"}
cat( paste( readLines( paste0(here(), "/NetworkPlots/CIMPlot_Combined.html") ), collapse="\n" ) )
```
<br>

There's three arguments in the **tar_combine** function in that plan, the first is just the target name, the second says what targets you want to combine, and the third (the command) says how you want to combine the targets. In my example, I named the new target *CompositeIntervalResults_Combined*, in the second argument, I specify the targets I want to combine by pulling them from the CIM_Mapped target. I do this by using list indexing, and selecting the first list element. Looking at the structure of the CIM_Mapped target might help to clarify this.

```{r Mapped_Structure, max.height='150px', echo = TRUE}

all_of(CIM_Mapped) %>% typeof()

all_of(CIM_Mapped) %>% str()

```

The third argument to **tar_combine** says how I want the targets to be combined. This uses an operator called "unquote splice" (!!!) which is used to unquote many arguments (targets). Honestly, I only started learning about these operators, and the larger metaprogramming family they belong to fairly recently. Using targets has actually been a great driving force practice these techniques. For a good introduction though, I thought the [chapters on metaprogramming](https://adv-r.hadley.nz/meta-big-picture.html) in the [Advanced R](https://adv-r.hadley.nz/index.html) were very helpful. [Chapter 19](https://adv-r.hadley.nz/quasiquotation.html) in particular covers the unquote-splice and related operators. 

Here I'll take a look at what this combined target looks like. 

<br>
```{r Combined_Structure, max.height='150px', echo = TRUE}

str(tar_read(CompositeIntervalResults_Combined))

```
<br>

You can see that this target is a simple list of the *cim* scanone objects, where each list element is named with its target name. This is very useful because the target names contain information about the model parameters used to create the output. This convention allows information about the parameters to be carried forward in the analysis. 

This pattern of defining many targets according to parameter options, and then combining the results of those many targets allows a user to quickly generate and keep track of many QTL mapping experiments. By making a single parameter table and calling two functions, you can have an organized list of QTL mapping results in hand and ready for comparison. 

### Other benefits of targets
The availability of tools like **tar_map** and **tar_combine** to improve workflow efficiency is arguably one of the least special aspects of targets. With some creative use of functional programming, anyone can mimic the same steps and get identical results with a standard script. Targets improves upon traditional workflows by providing tools that help you to massively reduce the cognitive load of these projects that can quickly become confusing, especially if you're new to mapping experiments (again, speaking from personal experience).

The [user manual](https://books.ropensci.org/targets/), (see chapters 1 and 4 especially), does a good job of justifying the use of the package.

### To do
- Visualizing results (ggplot, echarts, look into using shiny within markdown for more interactivity)
- MQM mapping
- Imputation step (for MQM mapping)
- Summary function (fitqtl)
- Parallel processing (Maybe HPC too at some point)

## Reproducibility

<details><summary>Reproducibility receipt</summary>

```{r}
## datetime
Sys.time()

## repository
if(requireNamespace('git2r', quietly = TRUE)) {
  git2r::repository()
} else {
  c(
    system2("git", args = c("log", "--name-status", "-1"), stdout = TRUE),
    system2("git", args = c("remote", "-v"), stdout = TRUE)
  )
}

## session info
sessionInfo()
```

</details>
